---
title: "Practical ML Project Writeup"
author: "Tejus"
date: "23/11/2020"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
library(caret); library(ggplot2); library(rattle); library(pheatmap); library(reshape2)
library(corrplot)
set.seed(825)
```

#### __Goal : __

To build a model for predicting the manner in which an individual did an exercise (the "class" variable)

#### __Introduction :__
Six young healthy participants were asked to perform one set of 10 repetitions of the Unilateral Dumbbell Biceps Curl in five different fashions (class A to class E). A key requirement for effective training to have a positive impact on cardio-respiratory fitness is a proper technique. Incorrect technique has been identified as the main cause of training injuries. The researchers in this work have tried to investigate the feasibility of automatically assessing the quality of execution of weight lifting exercises and the impact of providing real-time feedback to the athlete - so-called __qualitative activity recognition__.

The class variables have been methodically formulated to define the _quality_ of the exercise done. They are defined as follows :  

      * Exactly according to the specification (Class A)
      * Throwing the elbows to the front (Class B)
      * Lifting the dumbbell only halfway (Class C)
      * Lowering the dumbbell only halfway (Class D)
      * Throwing the hips to the front (Class E)

----

#### __Data loading and standardizing__

* Checked if any variable had values missing and removed `100` variables which had more than `r round((100/1.5),2)`% values NAs.

```{r LoadData, warning = FALSE, cache=TRUE, fig.align='center', fig.width=9, fig.height=8, echo=FALSE}

InputData <- as.data.frame(read.csv2("./Data/pml-training.csv", header = T, sep =",", quote = '"'))
trainIndex <- createDataPartition(y = InputData$classe, p = 0.70, list = FALSE)

trainData <- InputData[trainIndex,] ; testData <- InputData[-trainIndex,]
trainData <- trainData[,-(1:7)]

Cols <- colnames(trainData)[-153]
for(i in Cols){
   trainData[,i] <- as.numeric(as.character(trainData[,i]))
}

```

* All the values are standardized by centering around mean & dividing by standard deviation.  

* Outliers were detected and those values were imputed with `knn` method from _caret_

* Transformed the variables by `BoxCox` method from _caret_ 

* Also imputed the rest of small number of missing values with _knnImpute()_ method in `caret`


```{r Missing_Data, warning = FALSE, cache=TRUE, fig.align='center', fig.width=9, fig.height=8}

# Finding missing data
Blank_Cols <- apply(trainData[,-153], 2, function(Col)
      {S = sum(is.na(Col))#;print(S)
      if(S <= (length(Col)/1.5)) # Returns TRUE if the variable has <66% NAs else returns FALSE
                  {return(TRUE)}else{return(FALSE)}})

# Subsetting data with non missing/blank variables
trainData <- trainData[,c(Blank_Cols)]
#dim(trainData)

# Outlier detection
Indices <- apply(trainData[,-53], 2, function(Col){
                  IQR_var <- IQR(Col)
                  quantiles <- quantile(Col, probs = c(0.25,0.75), na.rm = T)
                  down <- quantiles[1] - (1.5*IQR_var)
                  up <- quantiles[2] + (1.5*IQR_var)
                  which(Col > up | Col < down)
})

# Replacing outliers with NA
for (i in 1:length(Indices)) {
   Index <- c(Indices[[i]])
   trainData[Index,i] <- NA
}

```


```{r BeforeNorm, warning = FALSE, cache=TRUE, fig.align='center', fig.width=9, fig.height=8,echo=FALSE}

# Before normalization
#trainData <- trainData.copy
trainData.melted <- melt(trainData, id = "classe", measure.vars = c(colnames(trainData[,-53])))
gg <- ggplot(aes(x = classe,y =value, group = classe), data = trainData.melted) + geom_boxplot(aes(fill=classe)) + facet_wrap(~variable) + labs(title = "Variables Boxplot prior Normalization") 
plot(gg)

```



```{r normalize, echo=TRUE, warning=FALSE, cache=TRUE}
# Impute any remaining missing values and standardize
preProcObj <- preProcess(trainData, method = c("knnImpute","BoxCox", "center", "scale"))
trainData <- data.frame(predict(preProcObj, trainData))
trainData.copy <- trainData
# trainData <- trainData.copy
```


```{r PostNorm, warning = FALSE, cache=TRUE, fig.align='center', fig.width=9, fig.height=8, echo=FALSE}

#trainData.copy <- trainData
Cols <- c(colnames(trainData)[-53])
# After normalization
trainData.melted <- melt(trainData, id = "classe", measure.vars = c(colnames(trainData[,-53])))

gg <- ggplot(aes(x = classe,y =value, group = classe), data = trainData.melted) + geom_boxplot(aes(fill=classe)) + facet_wrap(~variable) + labs(title = "Variables Boxplot post standardizing & normalization") 
plot(gg)

```

---- 

#### __Exploratory Analysis : __

* __Near Zero Variance Variables__ : Removing variables with low contribution to output variation.

```{r remZeroVar, cache=TRUE}
nZero_Predictors <- nearZeroVar(trainData, saveMetrics = T)
sum(nZero_Predictors$nzv == TRUE)
```

   * Number of variables with near zero variance contribution is zero. Hence we retain them all. 

* __Correlation analysis__ : Finding features with high correlation with each other.

```{r VariableCor, cache=TRUE, fig.align='center', fig.width=7, fig.height=7, echo=FALSE}
trainData$classe <- as.factor(trainData$classe)
Cor_Mat <- cor(trainData[,-53])
diag(Cor_Mat) <- 0 # Removing diag values

Cor_df <- data.frame(which(abs(Cor_Mat) > 0.8, arr.ind = T))
# Some highly correlated variable heatmap
#pheatmap(Cor_Mat[c(unique(Cor_df$row)),c(unique(Cor_df$col))])
corrplot(Cor_Mat, method = "color", type = "lower")

```

* Basic correlation analysis for finding variables that are highly correlated tells us that almost `r length(sort(unique(Cor_df$col)))` variables are highly correlated `(abs(cor_value) > 0.8)` with each other.

* We cannot simply remove the variable because that would result in creating a bias. So one way we can do this is choose to use PCA and use the variables from PCA for model building.

* We also can test a model which does `Boosting` like `bgm`. First let's create PCA meta-variables.

```{r PCA_Preprocess, cache=TRUE, fig.align='center', fig.width=8}
prCom <- preProcess(trainData, method = "pca")
trainData.PCA <- predict(prCom, newdata = trainData)

```

* Scree plots showing variance explained by each PCs. We can see that almost all the components are required for modeling.

```{r Scree, echo=FALSE, warning=FALSE, fig.align='center', fig.width=5, cache=TRUE}
Var <- apply(trainData.PCA[,-1], 2, var)
percentVar <- 100 * (round(Var / sum(Var),2))

plot(1:length(percentVar), percentVar, xlab="Principal Components", ylab="% Variance Explained", pch=16, type="o", col="red", main = "Scree Plot", ylim=c(1,100))
        lines(1:length(percentVar), cumsum(percentVar), type = "o", pch=16, col="dodgerblue")
        text(x=rep(10, 2), y=c(mean(percentVar)+10, mean(cumsum(percentVar))-10), pos=4, labels = c("Variance", "Cumulative Variance"), col = c("red", "dodgerblue"))
        
```

----

#### __Building a model : __

* We will try multiple models and see which one performs best.

```{r modelFits, cache=TRUE, warning=TRUE}
set.seed(825)
# Model function 1
mod_rPART <- train(classe~., data = trainData, method = "rpart") # rpart : R's pckg for partition trees
# Model function 2 ; A Boosting alternative
controlBM <- trainControl(method = "repeatedcv", number = 5, repeats = 1)

mod_GBM <- train(classe~., data = trainData, trControl = controlBM, method = "gbm", verbose=FALSE)

# Model function 2 ; Known best performer 1
controlRF <- trainControl(method="cv", number=3, verboseIter=FALSE)
mod_RF <- train(classe~., data = trainData, method = "rf", trControl= controlRF, allowParallel=TRUE)


 
```


#### Testing the model

* First we do the exact same transformations to the test data that we did to the training data.

```{r TestData, cache=TRUE, warning=FALSE}
# Subsetting
testData <- testData[,-(1:7)]; testClass <- as.factor(testData$classe)

Cols <- colnames(testData)[-153]
# Typecasting
for(i in Cols){
   testData[,i] <- as.numeric(as.character(testData[,i]))
}
# Subsetting data with non missing/blank variables
testData <- testData[,c(Blank_Cols)]

# Impute & scale
testData <- data.frame(predict(preProcObj, testData))

```


* Prediction for the test data to give an idea about out-of-sample prediction error rates & other metrics.


```{r Prediction, warning=FALSE}
#fancyRpartPlot(mod_rPART$finalModel, palettes=c("Greys", "Blues"))

pred_rPART <- predict(mod_rPART, newdata = testData)
pred_GBM <- predict(mod_GBM, newdata = testData)
pred_RF <- predict(mod_RF, newdata = testData)

T_rPART <- confusionMatrix(testClass, pred_rPART)
T_GBM <- confusionMatrix(pred_GBM,testClass)
T_rf <- confusionMatrix(testClass, pred_RF)


```


* __Sensitivity__

```{r Pred_Summary, echo=FALSE, cache=TRUE, warning=FALSE}

library(formattable)

customGreen0 = "#DeF7E9" ; customGreen = "#71CA97"
customRed0 = "#f5d3d3" ; customRed = "#ff7f7f"
customBlue0 = "#A5BDEE" ; customBlue = "#6995EE"

Models <- c("rPART", "Random Forest", "Gradient Boosting (GBM)")
Accuracy <- c(T_rPART$overall[1], T_rf$overall[1], T_GBM$overall[1])
Kappa_Est <- c(T_rPART$overall[2], T_rf$overall[2], T_GBM$overall[2])
Sensitivities <- data.frame(Models,rbind(t(T_rPART$byClass[,1]), (T_rf$byClass[,1]), (T_GBM$byClass[,1])))
Specifities <- data.frame(Models,rbind(t(T_rPART$byClass[,2]), t(T_rf$byClass[,2]), t(T_GBM$byClass[,2])))

Class <- paste("Class-",c(levels(testClass)), sep = "")
colnames(Sensitivities) <- c("Models", Class)
colnames(Specifities) <- c("Models", Class)

# Sensitivities
formattable(Sensitivities, 
               list("Models" = formatter("span", style = ~ style(color = "dodgerblue4",font.weight = "bold")),
               "Class-A" = color_tile(customBlue0, customBlue),
               "Class-B" = color_tile(customBlue0, customBlue),
               "Class-C" = color_tile(customBlue0, customBlue),
               "Class-D" = color_tile(customBlue0, customBlue),
               "Class-E" = color_tile(customBlue0, customBlue)
               ))

```

* __Specificity__

```{r Specificity, warning=FALSE, echo=FALSE, cache=TRUE}

# Specifities
formattable(Specifities, 
               list("Models" = formatter("span", style = ~ style(color = "dodgerblue4",font.weight = "bold")),        
               "Class-A" = color_tile(customBlue0, customBlue),
               "Class-B" = color_tile(customBlue0, customBlue),
               "Class-C" = color_tile(customBlue0, customBlue),
               "Class-D" = color_tile(customBlue0, customBlue),
               "Class-E" = color_tile(customBlue0, customBlue)
               ))

```

* __Accuracy & Cohen's Kappa __

```{r Overall_Summary,warning=FALSE, echo=FALSE, cache=TRUE}
# Overall Summary
df <- data.frame(Models, Accuracy, Kappa_Est)
formattable(df, 
            list("Models" = formatter("span", style = ~ style(color = "gray22",font.weight = "bold")),        "Accuracy" = color_tile(customGreen0, customGreen),
                 "Kappa_Est" = color_tile(customGreen0, customGreen) ))
```

* Overall if we use `Random Forest` we get `70%` accuracy which is my estimate of out-of-sample error rate.

---

#### __Predicting with the model for validation set__

* Again doing the same transformations & pre-processing as above

```{r Validation_Data, cache=TRUE, warning=FALSE}

Validation_Data <- as.data.frame(read.csv2("./Data/pml-testing.csv", header = T, sep =",", quote = '"'))
Validation_Data <- Validation_Data[,-(1:7)] ; Problem <- Validation_Data$problem_id

Cols <- colnames(Validation_Data)[-153]
for(i in Cols){
   Validation_Data[,i] <- as.numeric(as.character(Validation_Data[,i]))
}
# Subsetting data with non missing/blank variables
Validation_Data <- Validation_Data[,c(Blank_Cols)]

# Impute
Validation_Data <- data.frame(predict(preProcObj, Validation_Data))

```

* Prediction on validation data

```{r Prediction_Validation, cache=TRUE, warning=FALSE}

Val_pred_rPART <- predict(mod_rPART, newdata = Validation_Data)
Val_pred_GBM <- predict(mod_GBM, newdata = Validation_Data)
Val_pred_RF <- predict(mod_RF, newdata = Validation_Data)

```

* My prediction results

```{r Table, echo=FALSE, cache=TRUE}
temp <- data.frame(rbind( t(Val_pred_rPART), t(Val_pred_GBM), t(Val_pred_RF) ))
colnames(temp) <- Problem
Predictions <- data.frame(Models,temp)

formattable(Predictions)

```

---

### __Concluding remarks__

* Bootstrapping was done in all the models with `reps = 25`

* The known best performer __`Random Forest`__ performs better than other methods like `GBM & rPART` with an out of sample accuracy of __`87%`__

* The sensitivity & specificity of __`Random Forest`__ for each of the __`class`__ variables was also better than the other two methods.




